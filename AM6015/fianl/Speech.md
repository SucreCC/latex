# Start



Good morning everyone, this is Kai, here are my group members Anan and Laurel. Our topic is Modelling Biological Neural Networks.

In the following, I will introduce some basic knowledge about biological Neural and the history of Modelling Biological Neural Networks. Anan will talk about..., Laurel will talk about...





# 1 Background Context

In the first part, I will introduce some basic knowledge of biological neural, how nerve impulse works and what is Biological Neural network.

## Slide 1---Anatomy Of A Neural

Neurons are cells — small bodies of mostly water, ions, amino acids and proteins with remarkable electrochemical properties. They are the primary functional units of the brain.  

The nervous system is a net of neurons, each having a soma and an axon. Their adjunctions, or synapses, are always between the axon of one neuron and the soma of another.

Signals from the dendrites propagate to and converge at the soma, coming off the soma is the axon hillock which turns into the axon. The axon meets other neurons at synapses. 

# 

## Slide 2 ---Physiology Of Neural



Neural impulses are rapid electrical signals that travel along neurons in the nervous system, facilitating physiological and behavioral responses to stimuli.

The transmission of a nerve impulse include below stages: Polarization, Depolarization, Repolarization, Refractory Period.

点击换图

Unstimulated neurons belong to the resting state, The resting membrane potential is the non-excited state of a nerve cell at rest, the inside of the neuron is negatively charged relative to the outside. This voltage maintained at -70 mV. 

When a neuron receives sufficient stimulation, it undergoes a process called a nerve impulse, also known as an action potential. 

The process involves several key steps: depolarization, where the neuron's membrane potential becomes less negative; reaching a threshold, typically around -55 millivolts, triggering the action potential; 

the rising phase, characterized by a rapid increase in membrane potential due to the opening of sodium channels allowing sodium ions to enter the neuron; reaching the peak potential, usually around +30 millivolts; 

the falling phase, during which the membrane potential decreases as sodium channels close and potassium channels open, allowing potassium ions to leave the neuron; 

repolarization, where the membrane potential continues to decrease, eventually returning to its resting state; 

potential hyperpolarization, where the membrane potential briefly becomes more negative than the resting state; 

and finally, the recovery period, during which the membrane potential gradually returns to its resting state. These steps collectively enable neurons to transmit signals and information.



## Slide 3---Biological Neural Networks

Our biological neural networks are the complex systems of neurons and their connections in organisms, crucial for processing and transmitting information, enabling perception, decision-making, and behaviour. 

Do we have  a framework to study this complex systems? Absolutely we have.

In the first semester, I am confused about the course "Nonlinear Dynamics". To be honest, I think I will learn something like neural networks, machine learning algorithms, and statistics. But for this one,

it seems there is no strong relationship between the Dynamics of the System and machine learning. Until I did some study of "Modelling biological neural networks", I realised this course is a bridge which connects both carbon base neural and silicon base neural.

We use the knowledge of Nonlinear Dynamics to Model these networks through computational frameworks that aims to replicate their structure and function.





## 

## Slide 4---Spike Train



Spike trains are the language of neurons. People tend to think of spikes as point-events and spike trains as point-processe and assume spike trains are generated by random processes.



# 2 History Models

## Slide 5 -- The M-P Model

Providing threshold theory, if the sum of the received stimuli is greater than the threshold, it will output 1, otherwise it will output 0.

提供阀值理论，收到的刺激之和大于阀值就输出1，反之则输出0。

类似active function



The MP Neuron model, proposed by Warren McCulloch and Walter Pitts in 1943.

The MP Neuron model represents a binary threshold logic neuron, where inputs are binary (on/off) and weighted connections are summed. If the weighted sum exceeds a threshold, the neuron fires (outputs 1); otherwise, it does not fire (outputs 0).



**For example,** 

## Slide 6 --- Hebb Learning rules

When a neuron is connected to multiple neurons at the same time, the weight value between them can be selected according to the actual situation. If a certain connection is frequently activated, the weight between them can be larger.

一个神经元同时与多个神经元链接时，它们之间的权重值可以根据实际情况选择，如某一个连接经常被激活则它们之间的权重可以大一些。





In 1949, Donald Hebbinan introduced a neural learning rule, that shows the connection strength between Neurons increases when one neuron's firing consistently precedes another's, which means we can updates the connections or weights between neurons based on training samples.

Forbes (1939) mentions for example an estimate of 1300 synaptic knobs on a single anterior horn cell

## Slide 7 --- **Hodgkin-Huxley Model**

A circuit model is used to simulate the physiological behavior of cells under different conditions.

用一个电路模型用来模拟，细胞在不同条件下的生理行为。



two cell membrane

electrical circuit model 

channel

capacity



The Hodgkin–Huxley model, also known as the conductance-based model, was developed in 1952 by Alan Hodgkin and Andrew Huxley to elucidate the initiation and propagation of action potentials in neurons, representing a pioneering effort in mathematical neuroscience and electrical physiology. This model, consisting of nonlinear differential equations, approximates the electrical behavior of excitable cells like neurons and muscle cells, providing a continuous-time dynamical system framework for understanding neural excitability and signaling[3].

Over here is a tipical model of Hodgkin-Huxley.

- **Cm**: Membrane capacitance (Capacitance), indicating the ability of the cell membrane to store charge. 

- **gn(t,V)**: is a time- and voltage-dependent conductance, representing a gated channel of a specific ion (such as a sodium ion channel or a potassium ion channel).

- **En**: represents the Nernst potential of the corresponding ion, which is the potential difference on both sides of the cell membrane without net migration of the ion.

- **gL**: leakage conductance, which represents the conductivity of relatively constant ion channels, such as chloride ion channels, which are usually not controlled by voltage changes.

- **EL**: is the Nestor potential of the ion associated with gL.

- **Ip**: Injected current, which simulates the external stimulation received by the neuron, such as synaptic transmission from other neurons.





## Slide 8 --- Mark **I** Perceptron

随机连接的简单神经网络模型。



The Mark I Perceptron, developed by Frank Rosenblatt in 1957, was an early neural network model. It consisted of a single layer of neurons with binary threshold activation. The perceptron learned through a supervised learning algorithm, adjusting weights to minimize errors. Although successful for simple tasks, it had limitations, notably its inability to solve nonlinear problems.



Here is a diagram of Mark I perceptron,  it havs 3 layers: input layer, hidden layer and output layer.

We can see the nodes between input layer and hidden layers are randomly connect.

and the relationship between hidden layer and output layer is many to one.



## Slide 9 --- Delta Learning Rules

A common delta learning rule is a variant of the delta rule, called the delta rule or Widrow-Hoff rule. This rule is based on the perceptron model and updates the weights based on the difference between the actual output and the desired ouput.The size of the update depends on the size of the input signal as well as the error signal[5].

## Slide 10 Hopfield Neural Network

The Hopfield neural network, proposed by John Hopfield in 1982, is a feedback-type neural network used for pattern recognition and optimization problem solving. This fully connected neural network with symmetric connection weights utilizes an energy function to describe the stability of network states and employs asynchronous update rules. It can store and recognize a set of patterns, achieved during the training phase by adjusting connection weights. The Hopfield neural network finds widespread applications in fields such as pattern recognition, optimization problem solving, and image processing.



## Slide 11 Boltzmann Machine



The Boltzmann Machine, introduced in 1985 by Geoffrey Hinton and Terry Sejnowski, is a stochastic recurrent neural network inspired by statistical mechanics. It consists of binary neurons with symmetric connections and utilizes an energy-based model. Learning is achieved through Contrastive Divergence. Boltzmann Machines are used for tasks like feature learning and generative modeling, contributing to advancements in machine learning.

