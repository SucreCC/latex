@misc{161208242YOLO9000,
  title = {[1612.08242] {{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  urldate = {2024-02-28},
  howpublished = {https://arxiv.org/abs/1612.08242},
  file = {/Users/dengkai/Zotero/storage/6FVYU2W7/1612.html}
}

@article{A.L.Hodgkin1952QuantitativeDescriptionMembrane,
  title = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  author = {A. L. Hodgkin and A. F. Huxley, Pitts},
  year = {1952},
  month = aug,
  doi = {10.1113/jphysiol.1952.sp004764},
  file = {/Users/dengkai/Zotero/storage/WRGWQUMW/jphysiol01442-0106.pdf}
}

@article{ahunPerceptionsConcernsEmergency2023,
  title = {Perceptions and Concerns of Emergency Medicine Practitioners about Artificial Intelligence in Emergency Triage Management during the Pandemic: A National Survey-Based Study},
  shorttitle = {Perceptions and Concerns of Emergency Medicine Practitioners about Artificial Intelligence in Emergency Triage Management during the Pandemic},
  author = {Ahun, Erhan and Demir, Ahmet and Yi{\u g}it, Yavuz and Tulgar, Yasemin Ko{\c c}er and Do{\u g}an, Meltem and Thomas, David Terence and Tulgar, Serkan},
  year = {2023},
  month = oct,
  journal = {Frontiers in Public Health},
  volume = {11},
  pages = {1285390},
  issn = {2296-2565},
  doi = {10.3389/fpubh.2023.1285390},
  urldate = {2024-02-29},
  abstract = {Objective: There have been continuous discussions over the ethics of using AI in healthcare. We sought to identify the ethical issues and viewpoints of Turkish emergency care doctors about the use of AI during epidemic triage. Materials and methods: Ten emergency specialists were initially enlisted for this project, and their responses to open-ended questions about the ethical issues surrounding AI in the emergency room provided valuable information. A 15-question survey was created based on their input and was refined through a pilot test with 15 emergency specialty doctors. Following that, the updated survey was sent to emergency specialists via email, social media, and private email distribution. Results: 167 emergency medicine specialists participated in the study, with an average age of 38.22\, years and 6.79\, years of professional experience. The majority agreed that AI could benefit patients (54.50\%) and healthcare professionals (70.06\%) in emergency department triage during pandemics. Regarding responsibility, 63.47\% believed in shared responsibility between emergency medicine specialists and AI manufacturers/programmers for complications. Additionally, 79.04\% of participants agreed that the responsibility for complications in AI applications varies depending on the nature of the complication. Concerns about privacy were expressed by 20.36\% regarding deep learning-based applications, while 61.68\% believed that anonymity protected privacy. Additionally, 70.66\% of participants believed that AI systems would be as sensitive as humans in terms of non-discrimination. Conclusion: The potential advantages of deploying AI programs in emergency department triage during pandemics for patients and healthcare providers were acknowledged by emergency medicine doctors in Turkey. Nevertheless, they},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/E56BQE9K/Ahun et al. - 2023 - Perceptions and concerns of emergency medicine pra.pdf}
}

@article{almoallemTopEthicalIssues2020,
  title = {Top {{Ethical Issues Concerning Healthcare Providers Working}} in {{Saudi Arabia}}:},
  shorttitle = {Top {{Ethical Issues Concerning Healthcare Providers Working}} in {{Saudi Arabia}}},
  author = {Almoallem, Amar Mansour and Almudayfir, Mohammed Abdulaziz and {Al-Jahdail}, Yassar H. and Ahmed, Anwar E. and {Al-Shaikh}, Adnan and Baharoon, Salim and AlHarbi, Abdullah and {Al-Jahdali}, Hamdan},
  year = {2020},
  journal = {Journal of Epidemiology and Global Health},
  volume = {10},
  number = {2},
  pages = {143},
  issn = {2210-6014},
  doi = {10.2991/jegh.k.191211.001},
  urldate = {2024-02-29},
  abstract = {Background: Healthcare providers working in Saudi Arabia come from various nationalities, cultures, and training backgrounds. This study aimed to assess the perceptions of healthcare providers working in Riyadh hospitals about ethical dilemmas and solutions. Methods: This is a cross-sectional study among physicians working in Riyadh's private and governmental hospitals between June and December 2017. The study collected information on demographics, knowledge about medical ethics, the sources of such knowledge, and common ethical issues in general and the top ethical issues and dilemmas encountered in their daily practice. Results: A total of 455 physicians from government and private hospitals were enrolled in the study. The mean age of the participants was 34.29 {\textpm} 10.5 years, females were 29.7\% and mean years of practice was 13.0 {\textpm} 11.5. The top ethical issues identified by the participants were ``disagreement with the patients' relatives about treatment'' (91\%), patient disagreement with decisions made by professionals (84\%), treating the incompetent patient (79\%), conflict with administration policy and procedures (77\%), scarcity of resources (72\%), and making decision about do-not-resuscitate or life-sustaining treatment (68\%). There were significant differences in dealing with ethical issues in relation to gender, confidence about ethical knowledge, nationality, seniority, training site, and private or government hospitals academic and nonacademic. Conclusion: Healthcare providers in Riyadh hospitals face multiple ethical challenges. In addition to improvement in ethics knowledge through educational program among healthcare professional, there is a valid need for healthcare professionals and other sectors within society to engage in serious and continuous dialogue to address these issues and propose recommendations.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/WNJ9Q76D/Almoallem et al. - 2020 - Top Ethical Issues Concerning Healthcare Providers.pdf}
}

@misc{badrinarayananSegNetDeepConvolutional2016,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder-Decoder Architecture}} for {{Image Segmentation}}},
  shorttitle = {{{SegNet}}},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  year = {2016},
  month = oct,
  number = {arXiv:1511.00561},
  eprint = {1511.00561},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/dengkai/Zotero/storage/FGUWAX52/Badrinarayanan et al. - 2016 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf;/Users/dengkai/Zotero/storage/DZYJRL2G/1511.html}
}

@article{barabasiNETWORKSCIENCEPREFACE,
  title = {{{NETWORK SCIENCE PREFACE}}},
  author = {Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/T4KFK7F9/Barabási - NETWORK SCIENCE PREFACE.pdf}
}

@article{barabasiNETWORKSCIENCEPREFACEa,
  title = {{{NETWORK SCIENCE PREFACE}}},
  author = {Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/UENNXRQI/Barabási - NETWORK SCIENCE PREFACE.pdf}
}

@article{Bennasar2022SignificantFeaturesHuman,
  title = {Significant {{Features}} for {{Human Activity Recognition Using Tri-Axial Accelerometers}}},
  author = {Bennasar, Mohamed and Price, Blaine A. and Gooch, Daniel and Bandara, Arosha K. and Nuseibeh, Bashar},
  year = {2022},
  month = oct,
  journal = {Sensors},
  volume = {22},
  number = {19},
  pages = {7482},
  issn = {1424-8220},
  doi = {10.3390/s22197482},
  urldate = {2024-03-14},
  abstract = {Activity recognition using wearable sensors has become essential for a variety of applications. Tri-axial accelerometers are the most widely used sensor for activity recognition. Although various features have been used to capture patterns and classify the accelerometer signals to recognise activities, there is no consensus on the best features to choose. Reducing the number of features can reduce the computational cost and complexity and enhance the performance of the classifiers. This paper identifies the signal features that have significant discriminative power between different human activities. It also investigates the effect of sensor placement location, the sampling frequency, and activity complexity on the selected features. A comprehensive list of 193 signal features has been extracted from accelerometer signals of four publicly available datasets, including features that have never been used before for activity recognition. Feature significance was measured using the Joint Mutual Information Maximisation (JMIM) method. Common significant features among all the datasets were identified. The results show that the sensor placement location does not significantly affect recognition performance, nor does it affect the significant sub-set of features. The results also showed that with high sampling frequency, features related to signal repeatability and regularity show high discriminative power.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/V8EB689W/Bennasar et al. - 2022 - Significant Features for Human Activity Recognitio.pdf}
}

@article{BernardWidrow1960AdaptiveSwitchingCircuits,
  title = {Adaptive {{Switching Circuits}}},
  author = {Bernard Widrow and Marcian E. Hoff Jr.},
  year = {1960},
  file = {/Users/dengkai/Zotero/storage/HPXMLFY5/c1960adaptiveswitching.pdf}
}

@article{bredeNetworksIntroduction2012,
  title = {Networks--{{An Introduction}}},
  author = {Brede, Markus},
  year = 2012,
  journal = {Artificial Life},
  volume = {18},
  number = {2},
  pages = {241--242},
  publisher = {MIT Press},
  issn = {10645462},
  doi = {10.1162/artl_r_00062},
  urldate = {2023-09-12},
  abstract = {The article reviews the book "Networks: An Introduction," by Mark E. J. Newman .},
  keywords = {GRAPH theory,NETWORKS: An Introduction (Book),NEWMAN Mark E. J.,NONFICTION}
}

@article{Burr2017NeuromorphicComputingUsing,
  title = {Neuromorphic Computing Using Non-Volatile Memory},
  author = {Burr, Geoffrey W. and Shelby, Robert M. and Sebastian, Abu and Kim, Sangbum and Kim, Seyoung and Sidler, Severin and Virwani, Kumar and Ishii, Masatoshi and Narayanan, Pritish and Fumarola, Alessandro and Sanches, Lucas L. and Boybat, Irem and Le Gallo, Manuel and Moon, Kibong and Woo, Jiyoo and Hwang, Hyunsang and Leblebici, Yusuf},
  year = {2017},
  month = jan,
  journal = {Advances in Physics: X},
  volume = {2},
  number = {1},
  pages = {89--124},
  issn = {2374-6149},
  doi = {10.1080/23746149.2016.1259585},
  urldate = {2024-03-06},
  abstract = {Dense crossbar arrays of non-volatile memory (NVM) devices represent one possible path for implementing massively-parallel and highly energy-efficient neuromorphic computing systems. We first review recent advances in the application of NVM devices to three computing paradigms: spiking neural networks (SNNs), deep neural networks (DNNs), and `Memcomputing'. In SNNs, NVM synaptic connections are updated by a local learning rule such as spike-timing-dependent-plasticity, a computational approach directly inspired by biology. For DNNs, NVM arrays can represent matrices of synaptic weights, implementing the matrix--vector multiplication needed for algorithms such as backpropagation in an analog yet massively-parallel fashion. This approach could provide significant improvements in power and speed compared to GPU-based DNN training, for applications of commercial significance. We then survey recent research in which different types of NVM devices -- including phase change memory, conductive-bridging RAM, filamentary and nonfilamentary RRAM, and other NVMs -- have been proposed, either as a synapse or as a neuron, for use within a neuromorphic computing application. The relevant virtues and limitations of these devices are assessed, in terms of properties such as conductance dynamic range, (non)linearity and (a)symmetry of conductance response, retention, endurance, required switching power, and device variability.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/QNRQYEQX/Burr et al. - 2017 - Neuromorphic computing using non-volatile memory.pdf}
}

@misc{chenEncoderDecoderAtrousSeparable2018,
  title = {Encoder-{{Decoder}} with {{Atrous Separable Convolution}} for {{Semantic Image Segmentation}}},
  author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2018},
  month = aug,
  number = {arXiv:1802.02611},
  eprint = {1802.02611},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0{\textbackslash}\% and 82.1{\textbackslash}\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at {\textbackslash}url\{https://github.com/tensorflow/models/tree/master/research/deeplab\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/9WHGU4EH/Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution .pdf;/Users/dengkai/Zotero/storage/DI4VJNDY/1802.html}
}

@misc{chenRethinkingAtrousConvolution2017,
  title = {Rethinking {{Atrous Convolution}} for {{Semantic Image Segmentation}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2017},
  month = dec,
  number = {arXiv:1706.05587},
  eprint = {1706.05587},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  archiveprefix = {arxiv},
  copyright = {Seletive Reading},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/TIPPVP3Y/Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image S.pdf;/Users/dengkai/Zotero/storage/T8Z93IKL/1706.html}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/dengkai/Zotero/storage/3563TYU4/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf;/Users/dengkai/Zotero/storage/RPS5984J/2010.html}
}

@article{esmaeilzadehPatientsPerceptionsHuman2021,
  title = {Patients' {{Perceptions Toward Human}}--{{Artificial Intelligence Interaction}} in {{Health Care}}: {{Experimental Study}}},
  shorttitle = {Patients' {{Perceptions Toward Human}}--{{Artificial Intelligence Interaction}} in {{Health Care}}},
  author = {Esmaeilzadeh, Pouyan and Mirzaei, Tala and Dharanikota, Spurthy},
  year = {2021},
  month = nov,
  journal = {Journal of Medical Internet Research},
  volume = {23},
  number = {11},
  pages = {e25856},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/25856},
  urldate = {2024-02-29},
  abstract = {Background: It is believed that artificial intelligence (AI) will be an integral part of health care services in the near future and will be incorporated into several aspects of clinical care such as prognosis, diagnostics, and care planning. Thus, many technology companies have invested in producing AI clinical applications. Patients are one of the most important beneficiaries who potentially interact with these technologies and applications; thus, patients' perceptions may affect the widespread use of clinical AI. Patients should be ensured that AI clinical applications will not harm them, and that they will instead benefit from using AI technology for health care purposes. Although human-AI interaction can enhance health care outcomes, possible dimensions of concerns and risks should be addressed before its integration with routine clinical care. Objective: The main objective of this study was to examine how potential users (patients) perceive the benefits, risks, and use of AI clinical applications for their health care purposes and how their perceptions may be different if faced with three health care service encounter scenarios. Methods: We designed a 2{\texttimes}3 experiment that crossed a type of health condition (ie, acute or chronic) with three different types of clinical encounters between patients and physicians (ie, AI clinical applications as substituting technology, AI clinical applications as augmenting technology, and no AI as a traditional in-person visit). We used an online survey to collect data from 634 individuals in the United States. Results: The interactions between the types of health care service encounters and health conditions significantly influenced individuals' perceptions of privacy concerns, trust issues, communication barriers, concerns about transparency in regulatory standards, liability risks, benefits, and intention to use across the six scenarios. We found no significant differences among scenarios regarding perceptions of performance risk and social biases. Conclusions: The results imply that incompatibility with instrumental, technical, ethical, or regulatory values can be a reason for rejecting AI applications in health care. Thus, there are still various risks associated with implementing AI applications in diagnostics and treatment recommendations for patients with both acute and chronic illnesses. The concerns are also evident if the AI applications are used as a recommendation system under physician experience, wisdom, and control. Prior to the widespread rollout of AI, more studies are needed to identify the challenges that may raise concerns for implementing and using AI applications. This study could provide researchers and managers with critical insights into the determinants of individuals' intention to use AI clinical applications. Regulatory agencies should establish normative standards and evaluation guidelines for implementing AI in health care in cooperation with health care institutions. Regular audits and ongoing monitoring and reporting systems can be used to continuously evaluate the safety, quality, transparency, and ethical factors of AI clinical applications.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/MRTF54TE/Esmaeilzadeh et al. - 2021 - Patients’ Perceptions Toward Human–Artificial Inte.pdf;/Users/dengkai/Zotero/storage/QQPNTQQK/e25856.html}
}

@article{Farhud2021EthicalIssuesArtificial,
  title = {Ethical {{Issues}} of {{Artificial Intelligence}} in {{Medicine}} and {{Healthcare}}},
  author = {Farhud, Dariush D. and Zokaei, Shaghayegh},
  year = {2021},
  month = oct,
  journal = {Iranian Journal of Public Health},
  issn = {2251-6093, 2251-6085},
  doi = {10.18502/ijph.v50i11.7600},
  urldate = {2024-03-03},
  abstract = {The article's abstract is not available.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/B9GJFSQD/Farhud and Zokaei - 2021 - Ethical Issues of Artificial Intelligence in Medic.pdf}
}

@article{fossowambaResponsibleArtificialIntelligence2023,
  title = {Responsible {{Artificial Intelligence}} as a {{Secret Ingredient}} for {{Digital Health}}: {{Bibliometric Analysis}}, {{Insights}}, and {{Research Directions}}},
  shorttitle = {Responsible {{Artificial Intelligence}} as a {{Secret Ingredient}} for {{Digital Health}}},
  author = {Fosso Wamba, Samuel and Queiroz, Maciel M.},
  year = {2023},
  month = dec,
  journal = {Information Systems Frontiers},
  volume = {25},
  number = {6},
  pages = {2123--2138},
  issn = {1387-3326, 1572-9419},
  doi = {10.1007/s10796-021-10142-8},
  urldate = {2024-02-29},
  abstract = {With the unparallel advance of leading-edge technologies like artificial intelligence (AI), the healthcare systems are transforming and shifting for more digital health. In recent years, scientific productions have reached unprecedented levels. However, a holistic view of how AI is being used for digital health remains scarce. Besides, there is a considerable lack of studies on responsible AI and ethical issues that identify and suggest practitioners' essential insights towards the digital health domain. Therefore, we aim to rely on a bibliometric approach to explore the dynamics of the interplay between AI and digital health approaches, considering the responsible AI and ethical aspects of scientific production over the years. We found four distinct periods in the publication dynamics and the most popular approaches of AI in the healthcare field. Also, we highlighted the main trends and insightful directions for scholars and practitioners. In terms of contributions, this work provides a framework integrating AI technologies approaches and applications while discussing several barriers and benefits of AI-based health. In addition, five insightful propositions emerged as a result of the main findings. Thus, this study's originality is regarding the new framework and the propositions considering responsible AI and ethical issues on digital health.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/SVU3VWSQ/Fosso Wamba and Queiroz - 2023 - Responsible Artificial Intelligence as a Secret In.pdf}
}

@article{frostPublicViewsEthical2022,
  title = {Public Views on Ethical Issues in Healthcare Artificial Intelligence: Protocol for a Scoping Review},
  shorttitle = {Public Views on Ethical Issues in Healthcare Artificial Intelligence},
  author = {Frost, Emma Kellie and Bosward, Rebecca and Aquino, Yves Saint James and {Braunack-Mayer}, Annette and Carter, Stacy M.},
  year = {2022},
  month = dec,
  journal = {Systematic Reviews},
  volume = {11},
  number = {1},
  pages = {142},
  issn = {2046-4053},
  doi = {10.1186/s13643-022-02012-4},
  urldate = {2024-02-29},
  abstract = {Background:{\enspace} In recent years, innovations in artificial intelligence (AI) have led to the development of new health-care AI (HCAI) technologies. Whilst some of these technologies show promise for improving the patient experience, ethicists have warned that AI can introduce and exacerbate harms and wrongs in healthcare. It is important that HCAI reflects the values that are important to people. However, involving patients and publics in research about AI ethics remains challenging due to relatively limited awareness of HCAI technologies. This scoping review aims to map how the existing literature on publics' views on HCAI addresses key issues in AI ethics and governance. Methods:{\enspace} We developed a search query to conduct a comprehensive search of PubMed, Scopus, Web of Science, CINAHL, and Academic Search Complete from January 2010 onwards. We will include primary research studies which document publics' or patients' views on machine learning HCAI technologies. A coding framework has been designed and will be used capture qualitative and quantitative data from the articles. Two reviewers will code a proportion of the included articles and any discrepancies will be discussed amongst the team, with changes made to the coding framework accordingly. Final results will be reported quantitatively and qualitatively, examining how each AI ethics issue has been addressed by the included studies.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/4NEM75UZ/Frost et al. - 2022 - Public views on ethical issues in healthcare artif.pdf}
}

@article{G.E.Hinton1986LearningAndfRelearning,
  title = {Learning Andf {{Relearning}} in {{Boltzmann Machines}}},
  author = {G. E. Hinton and T. J. Sejnowski},
  year = {1986},
  volume = {1},
  pages = {282--317},
  file = {/Users/dengkai/Zotero/storage/3M2U8J7T/Learning_and_relearning_in_Boltzmann_machines.pdf}
}

@inproceedings{gasparinatouSpikingNeuralNetworks2023,
  title = {Spiking {{Neural Networks}} and {{Mathematical Models}}},
  booktitle = {{{GeNeDis}} 2022},
  author = {Gasparinatou, Mirto M. and Matzakos, Nikolaos and Vlamos, Panagiotis},
  editor = {Vlamos, Panagiotis},
  year = {2023},
  series = {Advances in {{Experimental Medicine}} and {{Biology}}},
  pages = {69--79},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-31982-2_8},
  abstract = {Neural networks are applied in various scientific fields such as medicine, engineering, pharmacology, etc. Investigating operations of neural networks refers to estimating the relationship among single neurons and their contributions to the network as well. Hence, studying a single neuron is an essential process to solve complex brain problems. Mathematical models that simulate neurons and the way they transmit information are proven to be an indispensable tool for neuroscientists. Constructing appropriate mathematical models to simulate information transmission of a biological neural network is a challenge for researchers, as in the real world, identical neurons in terms of their electrophysiological characteristics in different brain regions do not contribute in the same way to information transmission within a neural network due to the intrinsic characteristics. This review highlights four mathematical, single-compartment models: Hodgkin-Huxley, Izhikevich, Leaky Integrate, and Fire and Morris-Lecar, and discusses comparison among them in terms of their biological plausibility, computational complexity, and applications, according to modern literature.},
  isbn = {978-3-031-31982-2},
  langid = {english},
  keywords = {Hodgkin-Huxley,Izhikevich,Leaky integrate and fire,Mathematical models,Morris-Lecar,Neural networks,Single-compartment model}
}

@misc{girshickFastRCNN2015,
  title = {Fast {{R-CNN}}},
  author = {Girshick, Ross},
  year = {2015},
  month = sep,
  number = {arXiv:1504.08083},
  eprint = {1504.08083},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1504.08083},
  urldate = {2024-02-28},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/IV73V9SW/Girshick - 2015 - Fast R-CNN.pdf;/Users/dengkai/Zotero/storage/SHI342F3/1504.html}
}

@misc{girshickRichFeatureHierarchies2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  month = oct,
  number = {arXiv:1311.2524},
  eprint = {1311.2524},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1311.2524},
  urldate = {2024-02-28},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/SX2FX4RM/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detec.pdf;/Users/dengkai/Zotero/storage/VNKPAUAZ/1311.html}
}

@article{Gomaa2023PerspectiveHumanActivity,
  title = {A Perspective on Human Activity Recognition from Inertial Motion Data},
  author = {Gomaa, Walid and Khamis, Mohamed A.},
  year = {2023},
  month = oct,
  journal = {Neural Computing and Applications},
  volume = {35},
  number = {28},
  pages = {20463--20568},
  issn = {0941-0643, 1433-3058},
  doi = {10.1007/s00521-023-08863-9},
  urldate = {2024-03-14},
  abstract = {Human activity recognition (HAR) using inertial motion data has gained a lot of momentum in recent years both in research and industrial applications. From the abstract perspective, this has been driven by the rapid dynamics for building intelligent, smart environments, and ubiquitous systems that cover all aspects of human life including healthcare, sports, manufacturing, commerce, etc., which necessitate and subsume activity recognition aiming at recognizing the actions, characteristics, and goals of one or more agent(s) from a temporal series of observations streamed from one or more sensors. From a more concrete and seemingly orthogonal perspective, such momentum has been driven by the ubiquity of inertial motion sensors on-board mobile and wearable devices including smartphones, smartwatches, etc. In this paper we give an introductory and a comprehensive survey to the subject from a given perspective. We focus on a subset of topics, that we think are major, that will have significant and influential impacts on the future research and industrial-scale deployment of HAR systems. These include: (1) a comprehensive and detailed description of the inertial motion benchmark datasets that are publicly available and/or accessible, (2) feature selection and extraction techniques and the corresponding learning methods used to build workable HAR systems; we survey classical handcrafted datasets as well as data-oriented automatic representation learning approach to the subject, (3) transfer learning as a way to overcome many hurdles in actual deployments of HAR systems on a large scale, (4) embedded implementations of HAR systems on mobile and/or wearable devices, and finally (5) we touch on adversarial attacks, a topic that is essentially related to the security and privacy of HAR systems. As the field is very huge and diverse, this article is by no means comprehensive; it is though meant to provide a logically and conceptually rather complete picture to advanced practitioners, as well as to present a readable guided introduction to newcomers. Our logical and conceptual perspectives mimic the typical data science pipeline for state-of-the-art AI-based systems.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/D2KA7BTU/Gomaa and Khamis - 2023 - A perspective on human activity recognition from i.pdf}
}

@misc{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arxiv},
  copyright = {Skimming},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/dengkai/Zotero/storage/3AKLIJVQ/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/Users/dengkai/Zotero/storage/3JBJNJTS/1406.html}
}

@book{Hebb2002OrganizationBehaviorNeuropsychological,
  title = {The Organization of Behavior: A Neuropsychological Theory},
  shorttitle = {The Organization of Behavior},
  author = {Hebb, D. O.},
  year = {2002},
  publisher = {L. Erlbaum Associates},
  address = {Mahwah, N.J},
  isbn = {978-0-8058-4300-2},
  langid = {english},
  lccn = {BF181 .H4 2002},
  keywords = {Neuropsychology},
  file = {/Users/dengkai/Zotero/storage/P2C5FCMF/Hebb - 2002 - The organization of behavior a neuropsychological.pdf}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8{\texttimes} deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/8AAKM6VL/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf}
}

@incollection{Hinton2001LearningRelearningBoltzmanna,
  title = {Learning and {{Relearning}} in {{Boltzmann Machines}}},
  booktitle = {Graphical {{Models}}},
  author = {Hinton, G. E. and Sejnowski, T. J.},
  editor = {Jordan, Michael I. and Sejnowski, Terrence J.},
  year = {2001},
  month = oct,
  pages = {45--76},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/3349.003.0005},
  urldate = {2024-03-10},
  isbn = {978-0-262-29120-0},
  langid = {english}
}

@misc{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  month = mar,
  number = {arXiv:1503.02531},
  eprint = {1503.02531},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1503.02531},
  urldate = {2024-02-28},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arxiv},
  copyright = {Seletive Reading},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/dengkai/Zotero/storage/ZACI2DHV/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf;/Users/dengkai/Zotero/storage/Q8YJTQT8/1503.html}
}

@misc{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  year = {2017},
  month = apr,
  number = {arXiv:1704.04861},
  eprint = {1704.04861},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1704.04861},
  urldate = {2024-02-28},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/ZQ79E28N/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf;/Users/dengkai/Zotero/storage/T6XJ49E2/1704.html}
}

@misc{howardSearchingMobileNetV32019,
  title = {Searching for {{MobileNetV3}}},
  author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
  year = {2019},
  month = nov,
  number = {arXiv:1905.02244},
  eprint = {1905.02244},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2{\textbackslash}\% more accurate on ImageNet classification while reducing latency by 15{\textbackslash}\% compared to MobileNetV2. MobileNetV3-Small is 4.6{\textbackslash}\% more accurate while reducing latency by 5{\textbackslash}\% compared to MobileNetV2. MobileNetV3-Large detection is 25{\textbackslash}\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30{\textbackslash}\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.},
  archiveprefix = {arxiv},
  copyright = {Skimming},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/BHJI8NXU/Howard et al. - 2019 - Searching for MobileNetV3.pdf;/Users/dengkai/Zotero/storage/AFSXURHY/1905.html}
}

@inproceedings{huangDenselyConnectedConvolutional2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
  year = {2017},
  month = jul,
  pages = {2261--2269},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.243},
  urldate = {2024-02-28},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/WVVL4LLI/Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf}
}

@article{iniestaHumanRoleGuarantee2023,
  title = {The Human Role to Guarantee an Ethical {{AI}} in Healthcare: A Five-Facts Approach},
  shorttitle = {The Human Role to Guarantee an Ethical {{AI}} in Healthcare},
  author = {Iniesta, Raquel},
  year = {2023},
  month = oct,
  journal = {AI and Ethics},
  issn = {2730-5953, 2730-5961},
  doi = {10.1007/s43681-023-00353-x},
  urldate = {2024-02-29},
  abstract = {With the emergence of AI systems to assist clinical decision-making, several ethical dilemmas are brought to the general attention. AI systems are claimed to be the solution for many high-skilled medical tasks where machines can potentially surpass human ability as for example in identifying normal and abnormal chest X-rays. However, there are also warns that AI tools could be the basis for a human replacement that can risk dehumanisation in medicine. In recent years, important proposals in the domain of AI ethics in healthcare have identified main ethical issues, as for example fairness, autonomy, transparency, and responsibility. The human warranty, which implies human evaluation of the AI procedures, has been described to lower the ethical risks. However, as relevant these works have been, translating principles into action has proved challenging as existing codes were mostly a description of principles. There is a great need to produce how-to proposals that are specific enough to be action-guiding. We present five human-focussed facts designed into a framework of human action for an ethical AI in healthcare. Through the factors, we examine the role of medical practitioners, patients, and developers in designing, implementing, and using AI in a responsible manner that preserves human dignity. The facts encompass a range of ethical concerns that were commonly found in relevant literature. Given that it is crucial to bring as many perspectives as possible to the field, this work contributes to translate principles into human action to guarantee an ethical AI in health.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/YITJ8Q4J/Iniesta - 2023 - The human role to guarantee an ethical AI in healt.pdf}
}

@misc{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = mar,
  number = {arXiv:1502.03167},
  eprint = {1502.03167},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.03167},
  urldate = {2024-02-28},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/dengkai/Zotero/storage/F36TSZYV/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf;/Users/dengkai/Zotero/storage/KDGBFGES/1502.html}
}

@article{JayasundaraETHICALISSUESSURROUNDING,
  title = {{{ETHICAL ISSUES SURROUNDING THE USE OF INFORMATION IN HEALTH CARE}}},
  author = {Jayasundara, Chaminda Chiran},
  abstract = {As a result of rapid technological and economic expansion throughout the world, society is confronted with new requirements. For the success of the medical practice, even with the rapid changes in technology and the medical field, practitioners involved in the use of patients' information are obliged to continue to behave ethically. This paper reviews the ethical challenges raised in the use of patients' information for medical and other purposes. It also discusses the values underlining the ethical issues and their importance in the use of patients' information in the doctor and patient context. The issues surrounding the use of patients' information such as secrecy and confidentiality are raised and potential problems in the area, policy issues which must be addressed by those concerned with the confidentiality and secrecy of health information and the germane legal issues are also discussed. Moreover, this is a review of the current status of the health care information ethics with particular reference to the United Kingdom, United States, Canada, Australia and developing countries. Finally, it concludes that the emerging field of health care information ethics will require careful thought and insights from an international collection of ethicists.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/9NH3ZL7X/Jayasundara - ETHICAL ISSUES SURROUNDING THE USE OF INFORMATION .pdf}
}

@article{JJHopfield1982NeuralNetworksPhysical,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities},
  author = {J J Hopfield},
  year = {1982},
  month = apr,
  doi = {10.1073/pnas.79.8.2554},
  file = {/Users/dengkai/Zotero/storage/6CAETPZ4/pnas00447-0135.pdf}
}

@article{katiraiEthicsAdvancingArtificial2023,
  title = {The Ethics of Advancing Artificial Intelligence in Healthcare: Analyzing Ethical Considerations for {{Japan}}'s Innovative {{AI}} Hospital System},
  shorttitle = {The Ethics of Advancing Artificial Intelligence in Healthcare},
  author = {Katirai, Amelia},
  year = {2023},
  month = jul,
  journal = {Frontiers in Public Health},
  volume = {11},
  pages = {1142062},
  issn = {2296-2565},
  doi = {10.3389/fpubh.2023.1142062},
  urldate = {2024-02-29},
  abstract = {Public and private investments into developing digital health technologies---including artificial intelligence (AI)---are intensifying globally. Japan is a key case study given major governmental investments, in part through a Cross-Ministerial Strategic Innovation Promotion Program (SIP) for an ``Innovative AI Hospital System.'' Yet, there has been little critical examination of the SIP Research Plan, particularly from an ethics approach. This paper reports on an analysis of the Plan to identify the extent to which it addressed ethical considerations set out in the World Health Organization's 2021 Guidance on the Ethics and Governance of Artificial Intelligence for Health. A coding framework was created based on the six ethical principles proposed in the Guidance and was used as the basis for a content analysis. 101 references to aspects of the framework were identified in the Plan, but attention to the ethical principles was found to be uneven, ranging from the strongest focus on the potential benefits of AI to healthcare professionals and patients (               n               \,=\,44; Principle 2), to no consideration of the need for responsive or sustainable AI (               n               \,=\,0; Principle 6). Ultimately, the findings show that the Plan reflects insufficient consideration of the ethical issues that arise from developing and implementing AI for healthcare purposes. This case study is used to argue that, given the ethical complexity of the use of digital health technologies, consideration of the full range of ethical concerns put forward by the WHO must urgently be made visible in future plans for AI in healthcare.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/A22MCYFV/Katirai - 2023 - The ethics of advancing artificial intelligence in.pdf}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  urldate = {2024-02-28},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/MDD2NKTG/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@article{Lake2023HumanlikeSystematicGeneralization,
  title = {Human-like Systematic Generalization through a Meta-Learning Neural Network},
  author = {Lake, Brenden M. and Baroni, Marco},
  year = {2023},
  month = nov,
  journal = {Nature},
  volume = {623},
  number = {7985},
  pages = {115--121},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-023-06668-3},
  urldate = {2024-03-12},
  abstract = {Abstract                            The power of human language and thought arises from systematic compositionality---the algebraic ability to understand and produce novel combinations from known components. Fodor and Pylyshyn               1               famously argued that artificial neural networks lack this capacity and are therefore not viable models of the mind. Neural networks have advanced considerably in the years since, yet the systematicity challenge persists. Here we successfully address Fodor and Pylyshyn's challenge by providing evidence that neural networks can achieve human-like systematicity when optimized for their compositional skills. To do so, we introduce the meta-learning for compositionality (MLC) approach for guiding training through a dynamic stream of compositional tasks. To compare humans and machines, we conducted human behavioural experiments using an~instruction learning paradigm. After considering seven different models, we found that, in contrast to perfectly systematic but rigid probabilistic symbolic models, and perfectly flexible but unsystematic neural networks, only MLC achieves both the systematicity and flexibility needed for human-like generalization. MLC also advances the compositional skills of machine learning systems in several systematic generalization benchmarks. Our results show how a standard neural network architecture, optimized for its compositional skills, can mimic human systematic generalization in a head-to-head comparison.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/XT5JXUPD/Lake and Baroni - 2023 - Human-like systematic generalization through a met.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  urldate = {2024-02-28},
  langid = {english},
  keywords = {Intensive Reading},
  file = {/Users/dengkai/Zotero/storage/AKYSTPGN/LeCun et al. - 2015 - Deep learning.pdf}
}

@misc{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  month = aug,
  number = {arXiv:2103.14030},
  eprint = {2103.14030},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/dengkai/Zotero/storage/TZ88BYZP/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf;/Users/dengkai/Zotero/storage/U7W5PSFV/2103.html}
}

@misc{longFullyConvolutionalNetworks2015,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  month = mar,
  number = {arXiv:1411.4038},
  eprint = {1411.4038},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/P54MMM5X/Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf;/Users/dengkai/Zotero/storage/FN9V596I/1411.html}
}

@article{Maass2002RealTimeComputingStable,
  title = {Real-{{Time Computing Without Stable States}}: {{A New Framework}} for {{Neural Computation Based}} on {{Perturbations}}},
  shorttitle = {Real-{{Time Computing Without Stable States}}},
  author = {Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  year = {2002},
  month = nov,
  journal = {Neural Computation},
  volume = {14},
  number = {11},
  pages = {2531--2560},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976602760407955},
  urldate = {2024-03-06},
  abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/KVADU2T9/Maass et al. - 2002 - Real-Time Computing Without Stable States A New F.pdf}
}

@article{Maitre2019BasicDailyActivity,
  title = {Basic {{Daily Activity Recognition}} with a {{Data Glove}}},
  author = {Maitre, Julien and Rendu, Cl{\'e}ment and Bouchard, K{\'e}vin and Bouchard, Bruno and Gaboury, S{\'e}bastien},
  year = {2019},
  journal = {Procedia Computer Science},
  volume = {151},
  pages = {108--115},
  issn = {18770509},
  doi = {10.1016/j.procs.2019.04.018},
  urldate = {2024-03-14},
  abstract = {Abstract Many people in the world are affected by the Alzheimer disease leading to the dysfunctionality of the hand. In one side, this Msymanpytopmeoipslenoint tthhee mwoosrtldimapreoratffanetctoefdthbiys tdhieseAaslezhaenidmenrotdimseuacshe alettaednitniogntois tghievednystfounthcitsioonnaeli.tyInotfhethoethhearnds.idIen, tohnee lsitiedrer,attuhries spyromvpidtoems tiwsonomtatihnesmolousttioinmspsourctahntasofcothmispudtiesreavsiesioanndannodtdmatuachgloatvteenatliloonwiisnggitvoenretcoogthniiszeonhea.nIdn gtehsetuortehserfosridvei,rttuhael lrietearlriatytuorer prorobvoitdicesaptwploicmatiaoinnss.oFlurotimonsthsisucfihndaisncgoamnpdutneeredv,iswioen daencdiddeadtatogldoevveelaollpoewdinogurtoowrencodgantaizeglohvanedprgoetsottuyrpees faollrovwiirntugaltoremaloitnyitoorr rthoeboetvicolauptipolnicoatfiothnes.dFyrsofumnctthiiosnafilnitdyinogf athned hnaenedd,byweredcoecgindiezdingtoodbejveecltospiendbaosuirc odwainlydaactativgiltoievse. Opruortoatpyppreoaacllhowisinsgimtpolem, ocnhietaopr t(h{$\sim$}2e2e0v\$o)luatnidoneffiofcitehnet d({$\sim$}y1s0fu0n\%ctoiofncaolritryecotfprthedeichtainodnsb)ycornesciodgenriinzigngthaotbwjeectasreinabbsatsriacctdinaiglyalalcthtievitthieeso.ryOaubroauptptrhoeagcehstiusrseimrepcloeg,ncihtieoanp. A({$\sim$}l2s2o0, \$w)eancdaneffiacccieesnst d({$\sim$}ir1e0c0tl\%y aonfdcoerarseiclyt ptroedthicetiroanws)dcaotan.sFidinerailnlyg, tthhaet pwreopaoreseadbsptrroatcotitnygpealilstdheestchreiboeryd ainboauwt tahye tgheastturereseraerccohgenristicoann. rAelpsroo,dwuceeciatn. access directly and easily to the raw data. Finally, the proposed prototype is described in a way that researchers can reproduce it.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/HV432R9M/Maitre et al. - 2019 - Basic Daily Activity Recognition with a Data Glove.pdf}
}

@article{Markram1997RegulationSynapticEfficacy,
  title = {Regulation of {{Synaptic Efficacy}} by {{Coincidence}} of {{Postsynaptic APs}} and {{EPSPs}}},
  author = {Markram, Henry and L{\"u}bke, Joachim and Frotscher, Michael and Sakmann, Bert},
  year = {1997},
  month = jan,
  journal = {Science},
  volume = {275},
  number = {5297},
  pages = {213--215},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.275.5297.213},
  urldate = {2024-03-06},
  abstract = {Activity-driven modifications in synaptic connections between neurons in the neocortex may occur during development and learning. In dual whole-cell voltage recordings from pyramidal neurons, the coincidence of postsynaptic action potentials (APs) and unitary excitatory postsynaptic potentials (EPSPs) was found to induce changes in EPSPs. Their average amplitudes were differentially up- or down-regulated, depending on the precise timing of postsynaptic APs relative to EPSPs. These observations suggest that APs propagating back into dendrites serve to modify single active synaptic connections, depending on the pattern of electrical activity in the pre- and postsynaptic neurons.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/RA3G6CWC/Markram et al. - 1997 - Regulation of Synaptic Efficacy by Coincidence of .pdf}
}

@article{martinez-martinEthicalIssuesUsing2021,
  title = {Ethical Issues in Using Ambient Intelligence in Health-Care Settings},
  author = {{Martinez-Martin}, Nicole and Luo, Zelun and Kaushal, Amit and Adeli, Ehsan and Haque, Albert and Kelly, Sara S and Wieten, Sarah and Cho, Mildred K and Magnus, David and {Fei-Fei}, Li and Schulman, Kevin and Milstein, Arnold},
  year = {2021},
  month = feb,
  journal = {The Lancet Digital Health},
  volume = {3},
  number = {2},
  pages = {e115-e123},
  issn = {25897500},
  doi = {10.1016/S2589-7500(20)30275-2},
  urldate = {2024-02-29},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/JW44U2PG/Martinez-Martin et al. - 2021 - Ethical issues in using ambient intelligence in he.pdf}
}

@misc{maShuffleNetV2Practical2018,
  title = {{{ShuffleNet V2}}: {{Practical Guidelines}} for {{Efficient CNN Architecture Design}}},
  shorttitle = {{{ShuffleNet V2}}},
  author = {Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
  year = {2018},
  month = jul,
  number = {arXiv:1807.11164},
  eprint = {1807.11164},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {Currently, the neural network architecture design is mostly guided by the {\textbackslash}emph\{indirect\} metric of computation complexity, i.e., FLOPs. However, the {\textbackslash}emph\{direct\} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical {\textbackslash}emph\{guidelines\} for efficient network design. Accordingly, a new architecture is presented, called {\textbackslash}emph\{ShuffleNet V2\}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/ZMYSWH3P/Ma et al. - 2018 - ShuffleNet V2 Practical Guidelines for Efficient .pdf;/Users/dengkai/Zotero/storage/4K47VCUM/1807.html}
}

@article{Mcculloch1854LOGICALCALCULUSIDEAS,
  title = {A {{LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY}}},
  author = {Mcculloch, Warren S and Pitts, Walter},
  year = {1854},
  month = sep,
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/2YW3NK38/Mcculloch and Pitts - A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOU.pdf}
}

@article{Mead1990NeuromorphicElectronicSystems,
  title = {Neuromorphic Electronic Systems},
  author = {Mead, C.},
  year = {Oct./1990},
  journal = {Proceedings of the IEEE},
  volume = {78},
  number = {10},
  pages = {1629--1636},
  issn = {00189219},
  doi = {10.1109/5.58356},
  urldate = {2024-03-06},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/IEZ8D3GE/Mead - 1990 - Neuromorphic electronic systems.pdf}
}

@misc{melas-kyriaziYouEvenNeed2021,
  title = {Do {{You Even Need Attention}}? {{A Stack}} of {{Feed-Forward Layers Does Surprisingly Well}} on {{ImageNet}}},
  shorttitle = {Do {{You Even Need Attention}}?},
  author = {{Melas-Kyriazi}, Luke},
  year = {2021},
  month = may,
  number = {arXiv:2105.02723},
  eprint = {2105.02723},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.02723},
  urldate = {2024-02-28},
  abstract = {The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9{\textbackslash}\% top-1 accuracy, compared to 77.9{\textbackslash}\% and 79.9{\textbackslash}\% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are.},
  archiveprefix = {arxiv},
  copyright = {Seletive Reading},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/Z9BUSQHF/Melas-Kyriazi - 2021 - Do You Even Need Attention A Stack of Feed-Forwar.pdf;/Users/dengkai/Zotero/storage/AVLL6S3K/2105.html}
}

@article{Meng2021ExplorationHumanActivity,
  title = {Exploration of {{Human Activity Recognition Using}} a {{Single Sensor}} for {{Stroke Survivors}} and {{Able-Bodied People}}},
  author = {Meng, Long and Zhang, Anjing and Chen, Chen and Wang, Xingwei and Jiang, Xinyu and Tao, Linkai and Fan, Jiahao and Wu, Xuejiao and Dai, Chenyun and Zhang, Yiyuan and Vanrumste, Bart and Tamura, Toshiyo and Chen, Wei},
  year = {2021},
  month = jan,
  journal = {Sensors},
  volume = {21},
  number = {3},
  pages = {799},
  issn = {1424-8220},
  doi = {10.3390/s21030799},
  urldate = {2024-03-14},
  abstract = {Commonly used sensors like accelerometers, gyroscopes, surface electromyography sensors, etc., which provide a convenient and practical solution for human activity recognition (HAR), have gained extensive attention. However, which kind of sensor can provide adequate information in achieving a satisfactory performance, or whether the position of a single sensor would play a significant effect on the performance in HAR are sparsely studied. In this paper, a comparative study to fully investigate the performance of the aforementioned sensors for classifying four activities (walking, tooth brushing, face washing, drinking) is explored. Sensors are spatially distributed over the human body, and subjects are categorized into three groups (able-bodied people, stroke survivors, and the union of both). Performances of using accelerometer, gyroscope, sEMG, and their combination in each group are evaluated by adopting the Support Vector Machine classifier with the Leave-One-Subject-Out Cross-Validation technique, and the optimal sensor position for each kind of sensor is presented based on the accuracy. Experimental results show that using the accelerometer could obtain the best performance in each group. The highest accuracy of HAR involving stroke survivors was 95.84 {\textpm} 1.75\% (mean {\textpm} standard error), achieved by the accelerometer attached to the extensor carpi ulnaris. Furthermore, taking the practical application of HAR into consideration, a novel approach to distinguish various activities of stroke survivors based on a pre-trained HAR model built on healthy subjects is proposed, the highest accuracy of which is 77.89 {\textpm} 4.81\% (mean {\textpm} standard error) with the accelerometer attached to the extensor carpi ulnaris.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/IQJYJZBG/Meng et al. - 2021 - Exploration of Human Activity Recognition Using a .pdf}
}

@misc{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arxiv},
  copyright = {Seletive Reading},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/dengkai/Zotero/storage/928U8T8K/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/Users/dengkai/Zotero/storage/ICNVDI2B/1301.html}
}

@inproceedings{Mongelli2023OpenAccessDatabase,
  title = {Open {{Access Database}} of {{Industry}} 4.0 {{Tasks}} for the {{Development}} of {{AI-Based Classifier}}},
  booktitle = {2023 {{Smart Systems Integration Conference}} and {{Exhibition}} ({{SSI}})},
  author = {Mongelli, Francesca and Menolotto, Matteo and O'Flynn, Brendan and Demarchi, Danilo},
  year = {2023},
  month = mar,
  pages = {1--5},
  publisher = {IEEE},
  address = {Brugge, Belgium},
  doi = {10.1109/SSI58917.2023.10387755},
  urldate = {2024-03-14},
  abstract = {Robots and humans coworkers are sharing more and more portions of the smart manufacturing globally, meeting the need for high flexibility and rapid changes in the production layout. To be fully effective, however, such transition from classic robotics to the so-called collaborative robotics has to address several open problems, mostly related with safety and task optimization. Promising answers are coming from the motion capture technology, where wearable and optoelectronic sensing devices are deployed to gather human centric data to provide the robots with some form of awareness respect with the human activity and position. Tracking the hand of the operator, in particular, offers many advantages as we use our hands to explore and interact with the surroundings and to communicate. This has been highlighted by the several works focusing on gesture hand configuration recognition. This work present HANDMI4, a new open access database of hand motion tracking data, which includes a wide range of static hand grasp configurations and some classic dynamic industry tasks. Such database was generated using two of the most mature technologies for motion capture: IMU-based data glove and camera-based triangulation. To test the capability of such dataset to foster AI-based task classifier, a set of machine learning techniques were implemented and tested. In particular, KNN weighted reached 94,4\% and 100\% of task classification accuracy for the data glove and the camera system, respectively. With this open access database we aim to boost the research around task classification through motion capture technology to enable the next revolution in smart manufacturing.},
  isbn = {9798350325065},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/7WMZM92N/Mongelli et al. - 2023 - Open Access Database of Industry 4.0 Tasks for the.pdf}
}

@article{NicolescuBestRomanianManagement,
  title = {The {{Best Romanian Management Studies}} 2019-2020},
  author = {Nicolescu, Ed Ovidiu and Oprean, Constantin and {\c T}{\^i}{\c t}u, Aurel-Mihail},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/2KGFHH7S/Nicolescu et al. - The Best Romanian Management Studies 2019-2020.pdf}
}

@article{onianiAdoptingExpandingEthical2023,
  title = {Adopting and Expanding Ethical Principles for Generative Artificial Intelligence from Military to Healthcare},
  author = {Oniani, David and Hilsman, Jordan and Peng, Yifan and Poropatich, Ronald K. and Pamplin, Jeremy C. and Legault, Gary L. and Wang, Yanshan},
  year = {2023},
  month = dec,
  journal = {npj Digital Medicine},
  volume = {6},
  number = {1},
  pages = {225},
  issn = {2398-6352},
  doi = {10.1038/s41746-023-00965-x},
  urldate = {2024-02-29},
  abstract = {Abstract             In 2020, the U.S. Department of Defense officially disclosed a set of ethical principles to guide the use of Artificial Intelligence (AI) technologies on future battlefields. Despite stark differences, there are core similarities between the military and medical service. Warriors on battlefields often face life-altering circumstances that require quick decision-making. Medical providers experience similar challenges in a rapidly changing healthcare environment, such as in the emergency department or during surgery treating a life-threatening condition. Generative AI, an emerging technology designed to efficiently generate valuable information, holds great promise. As computing power becomes more accessible and the abundance of health data, such as electronic health records, electrocardiograms, and medical images, increases, it is inevitable that healthcare will be revolutionized by this technology. Recently, generative AI has garnered a lot of attention in the medical research community, leading to debates about its application in the healthcare sector, mainly due to concerns about transparency and related issues. Meanwhile, questions around the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare. However, the ethical principles for generative AI in healthcare have been understudied. As a result, there are no clear solutions to address ethical concerns, and decision-makers often neglect to consider the significance of ethical principles before implementing generative AI in clinical practice. In an attempt to address these issues, we explore ethical principles from the military perspective and propose the ``GREAT PLEA'' ethical principles, namely Governability, Reliability, Equity, Accountability, Traceability, Privacy, Lawfulness, Empathy, and Autonomy for generative AI in healthcare. Furthermore, we introduce a framework for adopting and expanding these ethical principles in a practical way that has been useful in the military and can be applied to healthcare for generative AI, based on contrasting their ethical concerns and risks. Ultimately, we aim to proactively address the ethical dilemmas and challenges posed by the integration of generative AI into healthcare practice.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/H4HYTFMU/Oniani et al. - 2023 - Adopting and expanding ethical principles for gene.pdf}
}

@misc{PapersCodeModeling,
  title = {Papers with {{Code}} - {{Modeling}} Biological Face Recognition with Deep Convolutional Neural Networks},
  urldate = {2024-02-26},
  abstract = {No code available yet.},
  howpublished = {https://paperswithcode.com/paper/modeling-biological-face-recognition-with},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/WG8QLA9E/modeling-biological-face-recognition-with.html}
}

@article{Paraschiakos2020ActivityRecognitionUsing,
  title = {Activity Recognition Using Wearable Sensors for Tracking the Elderly},
  author = {Paraschiakos, Stylianos and Cachucho, Ricardo and Moed, Matthijs and Van Heemst, Diana and Mooijaart, Simon and Slagboom, Eline P. and Knobbe, Arno and Beekman, Marian},
  year = {2020},
  month = jul,
  journal = {User Modeling and User-Adapted Interaction},
  volume = {30},
  number = {3},
  pages = {567--605},
  issn = {0924-1868, 1573-1391},
  doi = {10.1007/s11257-020-09268-2},
  urldate = {2024-03-14},
  abstract = {A population group that is often overlooked in the recent revolution of self-tracking is the group of older people. This growing proportion of the general population is often faced with increasing health issues and discomfort. In order to come up with lifestyle advice towards the elderly, we need the ability to quantify their lifestyle, before and after an intervention. This research focuses on the task of activity recognition (AR) from accelerometer data. With that aim, we collect a substantial labelled dataset of older individuals wearing multiple devices simultaneously and performing a strict protocol of 16 activities (the GOTOV dataset, N = 28{$\mkern1mu$}). Using this dataset, we trained Random Forest AR models, under varying sensor set-ups and levels of activity description granularity. The model that combines ankle and wrist accelerometers (GENEActiv) produced the best results (accuracy {$>$} 80\%{$\mkern1mu$}) for 16-class classification. At the same time, when additional physiological information is used, the accuracy increased ({$>\mkern1mu$} 85\%{$\mkern1mu$}). To further investigate the role of granularity in our predictions, we developed the LARA algorithm, which uses a hierarchical ontology that captures prior biological knowledge to increase or decrease the level of activity granularity (merge classes). As a result, a 12-class model in which the different paces of walking were merged showed a performance above 93\%{$\mkern1mu$}. Testing this 12-class model in labelled free-living pilot data, the mean balanced accuracy appeared to be reasonably high, while using the LARA algorithm, we show that a 7-class model (lying down, sitting, standing, household, walking, cycling, jumping) was optimal for accuracy and granularity. Finally, we demonstrate the use of the latter model in unlabelled free-living data from a larger lifestyle intervention study. In this paper, we make the validation data as well as the derived prediction models available to the community.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/YQIW2249/Paraschiakos et al. - 2020 - Activity recognition using wearable sensors for tr.pdf}
}

@article{Prakash2022EthicalConundrumsApplication,
  title = {Ethical {{Conundrums}} in the {{Application}} of {{Artificial Intelligence}} ({{AI}}) in {{Healthcare}}---{{A Scoping Review}} of {{Reviews}}},
  author = {Prakash, Sreenidhi and Balaji, Jyotsna Needamangalam and Joshi, Ashish and Surapaneni, Krishna Mohan},
  year = {2022},
  month = nov,
  journal = {Journal of Personalized Medicine},
  volume = {12},
  number = {11},
  pages = {1914},
  issn = {2075-4426},
  doi = {10.3390/jpm12111914},
  urldate = {2024-03-03},
  abstract = {Background: With the availability of extensive health data, artificial intelligence has an inordinate capability to expedite medical explorations and revamp healthcare.Artificial intelligence is set to reform the practice of medicine soon. Despite the mammoth advantages of artificial intelligence in the medical field, there exists inconsistency in the ethical and legal framework for the application of AI in healthcare. Although research has been conducted by various medical disciplines investigating the ethical implications of artificial intelligence in the healthcare setting, the literature lacks a holistic approach. Objective: The purpose of this review is to ascertain the ethical concerns of AI applications in healthcare, to identify the knowledge gaps and provide recommendations for an ethical and legal framework. Methodology: Electronic databases Pub Med and Google Scholar were extensively searched based on the search strategy pertaining to the purpose of this review. Further screening of the included articles was done on the grounds of the inclusion and exclusion criteria. Results: The search yielded a total of 1238 articles, out of which 16 articles were identified to be eligible for this review. The selection was strictly based on the inclusion and exclusion criteria mentioned in the manuscript. Conclusion: Artificial intelligence (AI) is an exceedingly puissant technology, with the prospect of advancing medical practice in the years to come. Nevertheless, AI brings with it a colossally abundant number of ethical and legal problems associated with its application in healthcare. There are manifold stakeholders in the legal and ethical issues revolving around AI and medicine. Thus, a multifaceted approach involving policymakers, developers, healthcare providers and patients is crucial to arrive at a feasible solution for mitigating the legal and ethical problems pertaining to AI in healthcare.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/9J4LBN63/Prakash et al. - 2022 - Ethical Conundrums in the Application of Artificia.pdf}
}

@article{pulvermullerBiologicalConstraintsNeural2021,
  title = {Biological Constraints on Neural Network Models of Cognitive Function},
  author = {Pulverm{\"u}ller, Friedemann and Tomasello, Rosario and {Henningsen-Schomers}, Malte R. and Wennekers, Thomas},
  year = {2021},
  month = aug,
  journal = {Nature Reviews Neuroscience},
  volume = {22},
  number = {8},
  pages = {488--502},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-021-00473-5},
  urldate = {2024-02-26},
  abstract = {Neural network models are potential tools for improving our understanding of complex brain functions. To address this goal, these models need to be neurobiologically realistic. However, although neural networks have advanced dramatically in recent years and even achieve human-like performance on complex perceptual and cognitive tasks, their similarity to aspects of brain anatomy and physiology is imperfect. Here, we discuss different types of neural models, including localist, auto-associative and hetero-associative, deep and whole-brain networks, and identify aspects under which their biological plausibility can be improved. These aspects range from the choice of model neurons and of mechanisms of synaptic plasticity and learning, to implementation of inhibition and control, along with neuroanatomical properties including area structure and local and long-range connectivity. We highlight recent advances in developing biologically grounded cognitive theories and in mechanistically explaining, based on these brain-constrained neural models, hitherto unaddressed issues regarding the nature, localization and ontogenetic and phylogenetic development of higher brain functions. In closing, we point to possible future clinical applications of brain-constrained modelling.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/X6T7C8BK/Pulvermüller et al. - 2021 - Biological constraints on neural network models of.pdf}
}

@misc{radfordUnsupervisedRepresentationLearning2016,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  year = {2016},
  month = jan,
  number = {arXiv:1511.06434},
  eprint = {1511.06434},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  archiveprefix = {arxiv},
  copyright = {Seletive Reading},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/dengkai/Zotero/storage/WXJWDIFW/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf;/Users/dengkai/Zotero/storage/IGEIM5JS/1511.html}
}

@misc{redmonYOLO9000BetterFaster2016,
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  shorttitle = {{{YOLO9000}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  year = {2016},
  month = dec,
  number = {arXiv:1612.08242},
  eprint = {1612.08242},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/DZL4YF4U/Redmon and Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf;/Users/dengkai/Zotero/storage/28LV7M3B/1612.html}
}

@misc{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = may,
  number = {arXiv:1506.02640},
  eprint = {1506.02640},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02640},
  urldate = {2024-02-28},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/9CWJF3EH/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf;/Users/dengkai/Zotero/storage/8LEEKHMJ/1506.html}
}

@misc{renFasterRCNNRealTime2016,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2016},
  month = jan,
  number = {arXiv:1506.01497},
  eprint = {1506.01497},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.01497},
  urldate = {2024-02-28},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix = {arxiv},
  copyright = {Skimming},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/WRQE7V2J/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf;/Users/dengkai/Zotero/storage/YTIS4T2T/1506.html}
}

@book{ribeiroNetworkScience7th2022,
  title = {Network {{Science}}: 7th {{International Winter Conference}}, {{NetSci-X}} 2022, {{Porto}}, {{Portugal}}, {{February}} 8--11, 2022, {{Proceedings}}},
  shorttitle = {Network {{Science}}},
  editor = {Ribeiro, Pedro and Silva, Fernando and Mendes, Jos{\'e} Fernando and Laureano, Ros{\'a}rio},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13197},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-97240-0},
  urldate = {2023-09-12},
  isbn = {978-3-030-97239-4 978-3-030-97240-0},
  langid = {english},
  keywords = {artificial intelligence,communication networks,communication systems,complex networks,computer networks,computer systems,graph theory,machine learning,network architecture,network protocols,signal processing,social networks,telecommunication networks,telecommunication systems,theoretical computer science,wireless telecommunication systems},
  file = {/Users/dengkai/Zotero/storage/79NEN4M4/Ribeiro et al. - 2022 - Network Science 7th International Winter Conferen.pdf}
}

@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/Y8PWZM4I/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf;/Users/dengkai/Zotero/storage/6JRXUY68/1505.html}
}

@article{Rosenblatt1958PerceptronProbabilisticModel,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  urldate = {2024-03-18},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/MKXTRA6F/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf}
}

@misc{sandlerMobileNetV2InvertedResiduals2019,
  title = {{{MobileNetV2}}: {{Inverted Residuals}} and {{Linear Bottlenecks}}},
  shorttitle = {{{MobileNetV2}}},
  author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  year = {2019},
  month = mar,
  number = {arXiv:1801.04381},
  eprint = {1801.04381},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/R64DSK58/Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf;/Users/dengkai/Zotero/storage/7FQVLIIR/1801.html}
}

@misc{SearchArXivEprint,
  title = {Search {\textbar} {{arXiv}} E-Print Repository},
  urldate = {2024-02-28},
  howpublished = {https://arxiv.org/search/?query=Deep+residual+learning+for+image+recognition\&searchtype=title\&abstracts=show\&order=-announced\_date\_first\&size=50},
  file = {/Users/dengkai/Zotero/storage/9MRXWKF2/search.html}
}

@misc{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  number = {arXiv:1409.1556},
  eprint = {1409.1556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.1556},
  urldate = {2024-02-28},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/TLTJV6ID/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;/Users/dengkai/Zotero/storage/VLEFN6WC/1409.html}
}

@article{smallmanMultiScaleEthics2022,
  title = {Multi {{Scale Ethics}}---{{Why We Need}} to {{Consider}} the {{Ethics}} of {{AI}} in {{Healthcare}} at {{Different Scales}}},
  author = {Smallman, Melanie},
  year = {2022},
  month = dec,
  journal = {Science and Engineering Ethics},
  volume = {28},
  number = {6},
  pages = {63},
  issn = {1353-3452, 1471-5546},
  doi = {10.1007/s11948-022-00396-z},
  urldate = {2024-02-29},
  abstract = {Many researchers have documented how AI and data driven technologies have the potential to have profound effects on our lives---in ways that make these technologies stand out from those that went before. Around the world, we are seeing a significant growth in interest and investment in AI in healthcare. This has been coupled with rising concerns about the ethical implications of these technologies and an array of ethical guidelines for the use of AI and data in healthcare has arisen. Nevertheless, the question of if and how AI and data technologies can be ethical remains open to debate. This paper aims to contribute to this debate by considering the wide range of implications that have been attributed to these technologies and asking whether current ethical guidelines take these factors into account. In particular, the paper argues that while current ethics guidelines for AI in healthcare effectively account for the four key issues identified in the ethics literature (transparency; fairness; responsibility and privacy), they have largely neglected wider issues relating to the way in which these technologies shape institutional and social arrangements. This, I argue, has given current ethics guidelines a strong focus on evaluating the impact of these technologies on the individual, while not accounting for the powerful social shaping effects of these technologies. To address this, the paper proposes a Multiscale Ethics Framework, which aims to help technology developers and ethical evaluations to consider the wider implications of these technologies.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/8HQ6VXPE/Smallman - 2022 - Multi Scale Ethics—Why We Need to Consider the Eth.pdf}
}

@article{Smith2021ClinicalAIOpacity,
  title = {Clinical {{AI}}: Opacity, Accountability, Responsibility and Liability},
  shorttitle = {Clinical {{AI}}},
  author = {Smith, Helen},
  year = {2021},
  month = jun,
  journal = {AI \& SOCIETY},
  volume = {36},
  number = {2},
  pages = {535--545},
  issn = {0951-5666, 1435-5655},
  doi = {10.1007/s00146-020-01019-6},
  urldate = {2024-03-03},
  abstract = {The aim of this literature review was to compose a narrative review supported by a systematic approach to critically identify and examine concerns about accountability and the allocation of responsibility and legal liability as applied to the clinician and the technologist as applied the use of opaque AI-powered systems in clinical decision making. This review questions (a) if it is permissible for a clinician to use an opaque AI system (AIS) in clinical decision making and (b) if a patient was harmed as a result of using a clinician using an AIS's suggestion, how would responsibility and legal liability be allocated? Literature was systematically searched, retrieved, and reviewed from nine databases, which also included items from three clinical professional regulators, as well as relevant grey literature from governmental and non-governmental organisations. This literature was subjected to inclusion/exclusion criteria; those items found relevant to this review underwent data extraction. This review found that there are multiple concerns about opacity, accountability, responsibility and liability when considering the stakeholders of technologists and clinicians in the creation and use of AIS in clinical decision making. Accountability is challenged when the AIS used is opaque, and allocation of responsibility is somewhat unclear. Legal analysis would help stakeholders to understand their obligations and prepare should an undesirable scenario of patient harm eventuate when AIS were used.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/UZ48FUXX/Smith - 2021 - Clinical AI opacity, accountability, responsibili.pdf}
}

@misc{srinivasBottleneckTransformersVisual2021,
  title = {Bottleneck {{Transformers}} for {{Visual Recognition}}},
  author = {Srinivas, Aravind and Lin, Tsung-Yi and Parmar, Niki and Shlens, Jonathon and Abbeel, Pieter and Vaswani, Ashish},
  year = {2021},
  month = aug,
  number = {arXiv:2101.11605},
  eprint = {2101.11605},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4\% Mask AP and 49.7\% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7\% top-1 accuracy on the ImageNet benchmark while being up to 1.64x faster in compute time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/dengkai/Zotero/storage/CEEYFQQH/Srinivas et al. - 2021 - Bottleneck Transformers for Visual Recognition.pdf;/Users/dengkai/Zotero/storage/44YUTJDM/2101.html}
}

@article{stahlEthicsChatGPTExploring2024,
  title = {The Ethics of {{ChatGPT}} -- {{Exploring}} the Ethical Issues of an Emerging Technology},
  author = {Stahl, Bernd Carsten and Eke, Damian},
  year = {2024},
  month = feb,
  journal = {International Journal of Information Management},
  volume = {74},
  pages = {102700},
  issn = {02684012},
  doi = {10.1016/j.ijinfomgt.2023.102700},
  urldate = {2024-02-29},
  abstract = {This article explores ethical issues raised by generative conversational AI systems like ChatGPT. It applies established approaches for analysing ethics of emerging technologies to undertake a systematic review of possible benefits and concerns. The methodology combines ethical issues identified by Anticipatory Technology Ethics, Ethical Impact Assessment, and Ethical Issues of Emerging ICT Applications with AI-specific issues from the literature. These are applied to analyse ChatGPT's capabilities to produce humanlike text and interact seamlessly. The analysis finds ChatGPT could provide high-level societal and ethical benefits. However, it also raises significant ethical concerns across social justice, individual autonomy, cultural identity, and environ\- mental issues. Key high-impact concerns include responsibility, inclusion, social cohesion, autonomy, safety, bias, accountability, and environmental impacts. While the current discourse focuses narrowly on specific issues such as authorship, this analysis systematically uncovers a broader, more balanced range of ethical issues worthy of attention. Findings are consistent with emerging research and industry priorities on ethics of generative AI. Implications include the need for diverse stakeholder engagement, considering benefits and risks holistically when developing applications, and multi-level policy interventions to promote positive outcomes. Overall, the analysis demonstrates that applying established ethics of technology methodologies can produce a rigorous, comprehensive foundation to guide discourse and action around impactful emerging technologies like ChatGPT. The paper advocates sustaining this broad, balanced ethics perspective as use cases unfold to realize benefits while addressing ethical downsides.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/D2H3HKBM/Stahl and Eke - 2024 - The ethics of ChatGPT – Exploring the ethical issu.pdf}
}

@misc{szegedyGoingDeeperConvolutions2014,
  title = {Going {{Deeper}} with {{Convolutions}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2014},
  month = sep,
  number = {arXiv:1409.4842},
  eprint = {1409.4842},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.4842},
  urldate = {2024-02-28},
  abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/89MXMNSM/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf;/Users/dengkai/Zotero/storage/2W8NGFHU/1409.html}
}

@misc{szegedyInceptionv4InceptionResNetImpact2016,
  title = {Inception-v4, {{Inception-ResNet}} and the {{Impact}} of {{Residual Connections}} on {{Learning}}},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
  year = {2016},
  month = aug,
  number = {arXiv:1602.07261},
  eprint = {1602.07261},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.07261},
  urldate = {2024-02-28},
  abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
  archiveprefix = {arxiv},
  copyright = {Skimming},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/Q36ENCE4/Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of R.pdf;/Users/dengkai/Zotero/storage/VNARB4J5/1602.html}
}

@misc{szegedyRethinkingInceptionArchitecture2015,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  year = {2015},
  month = dec,
  number = {arXiv:1512.00567},
  eprint = {1512.00567},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error and 17.3\% top-1 error.},
  archiveprefix = {arxiv},
  copyright = {Seletive Reading},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/ZF9D9FWA/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer.pdf}
}

@misc{szegedyRethinkingInceptionArchitecture2015a,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  year = {2015},
  month = dec,
  number = {arXiv:1512.00567},
  eprint = {1512.00567},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error and 17.3\% top-1 error.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/ZK8BTVV8/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer.pdf}
}

@misc{tanEfficientNetRethinkingModel2020,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2020},
  month = sep,
  number = {arXiv:1905.11946},
  eprint = {1905.11946},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/dengkai/Zotero/storage/DAEHA2FS/Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf;/Users/dengkai/Zotero/storage/GRZQ4WVB/1905.html}
}

@misc{tanEfficientNetV2SmallerModels2021,
  title = {{{EfficientNetV2}}: {{Smaller Models}} and {{Faster Training}}},
  shorttitle = {{{EfficientNetV2}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2021},
  month = jun,
  number = {arXiv:2104.00298},
  eprint = {2104.00298},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy. With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3\% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0\% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/NI9ZJVGX/Tan and Le - 2021 - EfficientNetV2 Smaller Models and Faster Training.pdf;/Users/dengkai/Zotero/storage/L6VTEZS9/2104.html}
}

@article{Thapa2021PrecisionHealthData,
  title = {Precision Health Data: {{Requirements}}, Challenges and Existing Techniques for Data Security and Privacy},
  shorttitle = {Precision Health Data},
  author = {Thapa, Chandra and Camtepe, Seyit},
  year = {2021},
  month = feb,
  journal = {Computers in Biology and Medicine},
  volume = {129},
  pages = {104130},
  issn = {00104825},
  doi = {10.1016/j.compbiomed.2020.104130},
  urldate = {2024-03-02},
  abstract = {Precision health leverages information from various sources, including omics, lifestyle, environment, social media, medical records, and medical insurance claims to enable personalized care, prevent and predict illness, and precise treatments. It extensively uses sensing technologies (e.g., electronic health monitoring devices), computations (e.g., machine learning), and communication (e.g., interaction between the health data centers). As health data contain sensitive private information, including the identity of patient and carer and medical con\- ditions of the patient, proper care is required at all times. Leakage of these private information affects the personal life, including bullying, high insurance premium, and loss of job due to the medical history. Thus, the security, privacy of and trust on the information are of utmost importance. Moreover, government legislation and ethics committees demand the security and privacy of healthcare data. Besides, the public, who is the data source, always expects the security, privacy, and trust of their data. Otherwise, they can avoid contributing their data to the precision health system. Consequently, as the public is the targeted beneficiary of the system, the effectiveness of precision health diminishes. Herein, in the light of precision health data security, privacy, ethical and regulatory requirements, finding the best methods and techniques for the utilization of the health data, and thus precision health is essential. In this regard, firstly, this paper explores the regulations, ethical guidelines around the world, and domain-specific needs. Then it presents the requirements and investigates the associated challenges. Secondly, this paper investigates secure and privacy-preserving machine learning methods suitable for the computation of precision health data along with their usage in relevant health projects. Finally, it il\- lustrates the best available techniques for precision health data security and privacy with a conceptual system model that enables compliance, ethics clearance, consent management, medical innovations, and developments in the health domain.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/RA9Q3USQ/Thapa and Camtepe - 2021 - Precision health data Requirements, challenges an.pdf}
}

@misc{tolstikhinMLPMixerAllMLPArchitecture2021,
  title = {{{MLP-Mixer}}: {{An}} All-{{MLP Architecture}} for {{Vision}}},
  shorttitle = {{{MLP-Mixer}}},
  author = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  year = {2021},
  month = jun,
  number = {arXiv:2105.01601},
  eprint = {2105.01601},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.01601},
  urldate = {2024-02-28},
  abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/dengkai/Zotero/storage/DT7B99M4/Tolstikhin et al. - 2021 - MLP-Mixer An all-MLP Architecture for Vision.pdf;/Users/dengkai/Zotero/storage/DIKILQBR/2105.html}
}

@inproceedings{Torres-Sanchez20183DHandMotion,
  title = {A {{3D Hand Motion Capture Device}} with {{Haptic Feedback}} for {{Virtual Reality Applications}}},
  booktitle = {2018 {{IEEE Games}}, {{Entertainment}}, {{Media Conference}} ({{GEM}})},
  author = {{Torres-Sanchez}, Javier and Tedesco, Salvatore and O'Flynn, Brendan},
  year = {2018},
  month = aug,
  pages = {232--238},
  publisher = {IEEE},
  address = {Galway},
  doi = {10.1109/GEM.2018.8516460},
  urldate = {2024-03-14},
  abstract = {In this paper, the challenges associated with the design of new generation hand motion capture devices for Virtual Reality (VR) applications are described. The need for developing a hand motion capture device with tactile feedback that integrates all the sensors and actuators associated with VR, while meeting the latency requirements is introduced. A detailed description of functional and non-functional specifications is also given.},
  isbn = {978-1-5386-6304-2},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/HPGVJYJF/Torres-Sanchez et al. - 2018 - A 3D Hand Motion Capture Device with Haptic Feedba.pdf}
}

@misc{touvronTrainingDataefficientImage2021,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  year = {2021},
  month = jan,
  number = {arXiv:2012.12877},
  eprint = {2012.12877},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.12877},
  urldate = {2024-02-28},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/QTJXPDQD/Touvron et al. - 2021 - Training data-efficient image transformers & disti.pdf;/Users/dengkai/Zotero/storage/NME5FB54/2012.html}
}

@article{trocinResponsibleAIDigital2023,
  title = {Responsible {{AI}} for {{Digital Health}}: A {{Synthesis}} and a {{Research Agenda}}},
  shorttitle = {Responsible {{AI}} for {{Digital Health}}},
  author = {Trocin, Cristina and Mikalef, Patrick and Papamitsiou, Zacharoula and Conboy, Kieran},
  year = {2023},
  month = dec,
  journal = {Information Systems Frontiers},
  volume = {25},
  number = {6},
  pages = {2139--2157},
  issn = {1387-3326, 1572-9419},
  doi = {10.1007/s10796-021-10146-4},
  urldate = {2024-02-29},
  abstract = {Responsible AI is concerned with the design, implementation and use of ethical, transparent, and accountable AI technology in order to reduce biases, promote fairness, equality, and to help facilitate interpretability and explainability of outcomes, which are particularly pertinent in a healthcare context. However, the extant literature on health AI reveals significant issues regarding each of the areas of responsible AI, posing moral and ethical consequences. This is particularly concerning in a health context where lives are at stake and where there are significant sensitivities that are not as pertinent in other domains outside of health. This calls for a comprehensive analysis of health AI using responsible AI concepts as a structural lens. A systematic literature review supported our data collection and sampling procedure, the corresponding analysis, and extraction of research themes helped us provide an evidence-based foundation. We contribute with a systematic description and explanation of the intellectual structure of Responsible AI in digital health and develop an agenda for future research.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/7CS566I7/Trocin et al. - 2023 - Responsible AI for Digital Health a Synthesis and.pdf}
}

@article{Twomey2018ComprehensiveStudyActivity,
  title = {A {{Comprehensive Study}} of {{Activity Recognition Using Accelerometers}}},
  author = {Twomey, Niall and Diethe, Tom and Fafoutis, Xenofon and Elsts, Atis and McConville, Ryan and Flach, Peter and Craddock, Ian},
  year = {2018},
  month = may,
  journal = {Informatics},
  volume = {5},
  number = {2},
  pages = {27},
  issn = {2227-9709},
  doi = {10.3390/informatics5020027},
  urldate = {2024-03-14},
  abstract = {This paper serves as a survey and empirical evaluation of the state-of-the-art in activity recognition methods using accelerometers. The paper is particularly focused on long-term activity recognition in real-world settings. In these environments, data collection is not a trivial matter; thus, there are performance trade-offs between prediction accuracy, which is not the sole system objective, and keeping the maintenance overhead at minimum levels. We examine research that has focused on the selection of activities, the features that are extracted from the accelerometer data, the segmentation of the time-series data, the locations of accelerometers, the selection and configuration trade-offs, the test/retest reliability, and the generalisation performance. Furthermore, we study these questions from an experimental platform and show, somewhat surprisingly, that many disparate experimental configurations yield comparable predictive performance on testing data. Our understanding of these results is that the experimental setup directly and indirectly defines a pathway for context to be delivered to the classifier, and that, in some settings, certain configurations are more optimal than alternatives. We conclude by identifying how the main results of this work can be used in practice, specifically in experimental configurations in challenging experimental conditions.},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/HBXPL9UY/Twomey et al. - 2018 - A Comprehensive Study of Activity Recognition Usin.pdf}
}

@article{vandyckModelingBiologicalFace2023,
  title = {Modeling Biological Face Recognition with Deep Convolutional Neural Networks},
  author = {{van Dyck}, Leonard E. and Gruber, Walter R.},
  year = {2023},
  month = oct,
  journal = {Journal of Cognitive Neuroscience},
  volume = {35},
  number = {10},
  eprint = {2208.06681},
  primaryclass = {cs},
  pages = {1521--1537},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn_a_02040},
  urldate = {2024-02-26},
  abstract = {Deep convolutional neural networks (DCNNs) have become the state-of-the-art computational models of biological object recognition. Their remarkable success has helped vision science break new ground and recent efforts have started to transfer this achievement to research on biological face recognition. In this regard, face detection can be investigated by comparing face-selective biological neurons and brain areas to artificial neurons and model layers. Similarly, face identification can be examined by comparing in vivo and in silico multidimensional "face spaces". In this review, we summarize the first studies that use DCNNs to model biological face recognition. On the basis of a broad spectrum of behavioral and computational evidence, we conclude that DCNNs are useful models that closely resemble the general hierarchical organization of face recognition in the ventral visual pathway and the core face network. In two exemplary spotlights, we emphasize the unique scientific contributions of these models. First, studies on face detection in DCNNs indicate that elementary face selectivity emerges automatically through feedforward processing even in the absence of visual experience. Second, studies on face identification in DCNNs suggest that identity-specific experience and generative mechanisms facilitate this particular challenge. Taken together, as this novel modeling approach enables close control of predisposition (i.e., architecture) and experience (i.e., training data), it may be suited to inform long-standing debates on the substrates of biological face recognition.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/WGRY2LRT/van Dyck and Gruber - 2023 - Modeling biological face recognition with deep con.pdf;/Users/dengkai/Zotero/storage/4EQZADWM/2208.html}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/dengkai/Zotero/storage/AH79C7AW/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/Users/dengkai/Zotero/storage/DB6HLTQA/1706.html}
}

@inproceedings{Wozniak2016LearningSpatiotemporalPatterns,
  title = {Learning Spatio-Temporal Patterns in the Presence of Input Noise Using Phase-Change Memristors},
  booktitle = {2016 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Wozniak, Stanislaw and Tuma, Tomas and Pantazi, Angeliki and Eleftheriou, Evangelos},
  year = {2016},
  month = may,
  pages = {365--368},
  publisher = {IEEE},
  address = {Montr{\'e}al, QC, Canada},
  doi = {10.1109/ISCAS.2016.7527246},
  urldate = {2024-03-06},
  abstract = {Neuromorphic systems increasingly attract research interest owing to their ability to provide biologically inspired methods of computing, alternative to the classic von Neumann architecture. In these systems, computing relies on spike-based communication between neurons, and memory is represented by evolving states of the synaptic interconnections. In this work, we first demonstrate how spike-timing-dependent plasticity (STDP) based synapses can be realized using the crystal-growth dynamics of phase-change memristors. Then, we present a novel learning architecture comprising an integrate-and-fire neuron and an array of phase-change synapses that is capable of detecting temporal correlations in parallel input streams. We demonstrate a continuous re-learning operation on a sequence of binary 20{\texttimes}20 pixel images in the presence of significant background noise. Experimental results using an array of phase-change cells as synaptic elements confirm the functionality and performance of the proposed learning architecture.},
  isbn = {978-1-4799-5341-7},
  langid = {english},
  file = {/Users/dengkai/Zotero/storage/R9V9238H/Wozniak et al. - 2016 - Learning spatio-temporal patterns in the presence .pdf}
}

@misc{wuCvTIntroducingConvolutions2021,
  title = {{{CvT}}: {{Introducing Convolutions}} to {{Vision Transformers}}},
  shorttitle = {{{CvT}}},
  author = {Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  year = {2021},
  month = mar,
  number = {arXiv:2103.15808},
  eprint = {2103.15808},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.15808},
  urldate = {2024-02-28},
  abstract = {We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture ({\textbackslash}ie shift, scale, and distortion invariance) while maintaining the merits of Transformers ({\textbackslash}ie dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets ({\textbackslash}eg ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7{\textbackslash}\% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely removed in our model, simplifying the design for higher resolution vision tasks. Code will be released at {\textbackslash}url\{https://github.com/leoxiaobin/CvT\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/WQUF65RL/Wu et al. - 2021 - CvT Introducing Convolutions to Vision Transforme.pdf;/Users/dengkai/Zotero/storage/RZ2R977H/2103.html}
}

@misc{xieAggregatedResidualTransformations2017,
  title = {Aggregated {{Residual Transformations}} for {{Deep Neural Networks}}},
  author = {Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  year = {2017},
  month = apr,
  number = {arXiv:1611.05431},
  eprint = {1611.05431},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.05431},
  urldate = {2024-02-28},
  abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/CFNVNTLK/Xie et al. - 2017 - Aggregated Residual Transformations for Deep Neura.pdf;/Users/dengkai/Zotero/storage/XWPAPMIU/1611.html}
}

@misc{zhangMixupEmpiricalRisk2018,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle = {Mixup},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and {Lopez-Paz}, David},
  year = {2018},
  month = apr,
  number = {arXiv:1710.09412},
  eprint = {1710.09412},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/dengkai/Zotero/storage/LVV4X9ME/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf;/Users/dengkai/Zotero/storage/4GTEEQJX/1710.html}
}

@misc{zhangShuffleNetExtremelyEfficient2017,
  title = {{{ShuffleNet}}: {{An Extremely Efficient Convolutional Neural Network}} for {{Mobile Devices}}},
  shorttitle = {{{ShuffleNet}}},
  author = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  year = {2017},
  month = dec,
  number = {arXiv:1707.01083},
  eprint = {1707.01083},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8\%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves {\textasciitilde}13x actual speedup over AlexNet while maintaining comparable accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/DYJQBFK9/Zhang et al. - 2017 - ShuffleNet An Extremely Efficient Convolutional N.pdf;/Users/dengkai/Zotero/storage/Z3NHGBC6/1707.html}
}

@misc{zhaoPyramidSceneParsing2017,
  title = {Pyramid {{Scene Parsing Network}}},
  author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  year = {2017},
  month = apr,
  number = {arXiv:1612.01105},
  eprint = {1612.01105},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1612.01105},
  urldate = {2024-02-28},
  abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dengkai/Zotero/storage/VDK4M43E/Zhao et al. - 2017 - Pyramid Scene Parsing Network.pdf;/Users/dengkai/Zotero/storage/VAMYULCI/1612.html}
}

@misc{zophLearningTransferableArchitectures2018,
  title = {Learning {{Transferable Architectures}} for {{Scalable Image Recognition}}},
  author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
  year = {2018},
  month = apr,
  number = {arXiv:1707.07012},
  eprint = {1707.07012},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4\% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/dengkai/Zotero/storage/4K85H6ZA/Zoph et al. - 2018 - Learning Transferable Architectures for Scalable I.pdf;/Users/dengkai/Zotero/storage/ILL9ME6H/1707.html}
}

@misc{zophNeuralArchitectureSearch2017,
  title = {Neural {{Architecture Search}} with {{Reinforcement Learning}}},
  author = {Zoph, Barret and Le, Quoc V.},
  year = {2017},
  month = feb,
  number = {arXiv:1611.01578},
  eprint = {1611.01578},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
  archiveprefix = {arxiv},
  copyright = {Skimming},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/dengkai/Zotero/storage/NKQG4DLA/Zoph and Le - 2017 - Neural Architecture Search with Reinforcement Lear.pdf;/Users/dengkai/Zotero/storage/7X4SSY4L/1611.html}
}
